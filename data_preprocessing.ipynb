{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "filename = 'twitter_cs'\n",
    "\n",
    "twcs: pd.DataFrame = pd.read_csv(f'data/{filename}.csv', nrows=100_000)\n",
    "twcs = twcs.sort_values('created_at')\n",
    "twcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twcs[\"text\"] = (\n",
    "    twcs[\"text\"]\n",
    "    .str.replace(r\"^\\s*@[^ ]*\", \"\", regex=True)\n",
    "    .str.replace(r\"https?:\\/\\/[^\\s\\\\n]+\", \"\", regex=True)\n",
    "    .str.replace(r\"\\n+\", ' ', regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "twcs = twcs.rename(columns={'inbound': 'is_customer'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root(tweet_id, df):\n",
    "    parent_id = tweet_id\n",
    "\n",
    "    while True:\n",
    "        potential_parent = df[df['in_response_to_tweet_id'] == parent_id]['tweet_id']\n",
    "\n",
    "        if len(potential_parent) == 0:\n",
    "            return parent_id\n",
    "        \n",
    "        parent_id = potential_parent.values[0]\n",
    "\n",
    "tqdm.pandas(desc=\"Making threads...\")\n",
    "twcs['thread_id'] = twcs['tweet_id'].progress_apply(lambda x: find_root(x, twcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate to chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_conversations(df):\n",
    "    altnerating_messages = []\n",
    "    prev_is_customer = None\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        is_customer = row['is_customer']\n",
    "\n",
    "        if prev_is_customer is None and not is_customer: # Always start with a customer message\n",
    "            continue\n",
    "        elif prev_is_customer == is_customer:\n",
    "            altnerating_messages[-1] += ' ' + row['text']\n",
    "        else:\n",
    "            altnerating_messages.append(row['text'])\n",
    "            prev_is_customer = is_customer\n",
    "\n",
    "    return altnerating_messages\n",
    "\n",
    "chats_df = twcs.copy()\n",
    "tqdm.pandas(desc=\"Grouping conversations...\")\n",
    "chats_df['chat'] = chats_df.progress_apply(lambda x: group_conversations(twcs[twcs['thread_id'] == x['thread_id']]), axis=1)\n",
    "chats_df = chats_df.drop_duplicates('thread_id')[['chat']]\n",
    "chats_df['n_messages'] = chats_df['chat'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_length = chats_df['n_messages'] >= 4\n",
    "non_dm = chats_df['chat'].apply(lambda c: all([' dm' not in m.lower() for m in c]))\n",
    "\n",
    "chats_df = chats_df[proper_length & non_dm]\n",
    "print(f\"Found {len(chats_df)} fitting chats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed random question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_customer_idx(x):\n",
    "    even = [i for i in range(0, x - 2, 2)] # Don't pick the last message\n",
    "    return np.random.choice(even)\n",
    "\n",
    "chats_df['aug_idx'] = chats_df['n_messages'].apply(lambda x: random_customer_idx(x))\n",
    "chats_df['aug_text'] = chats_df.apply(lambda x: x['chat'][x['aug_idx']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    # \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "    \"TaylorAI/bge-micro-v2\"\n",
    ")\n",
    "\n",
    "to_embed = chats_df['aug_text'].tolist()\n",
    "embeddings = model.encode(to_embed, show_progress_bar=True)\n",
    "chats_df['aug_embedding'] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ks = [i for i in range(1, 50, 1)]\n",
    "wcss = []\n",
    "\n",
    "for k in ks:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(embeddings)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "\n",
    "plt.plot(ks, wcss)\n",
    "\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('WCSS (Inertia)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(embeddings)\n",
    "chats_df['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitue_sim_answer(row, df):\n",
    "    same_cluster  = df[df['cluster'] == row['cluster']]\n",
    "\n",
    "    sub = same_cluster.loc[np.random.choice(same_cluster.index)]\n",
    "    sub_chat, sub_aug_idx= sub['chat'], sub['aug_idx']\n",
    "    sub_answer = sub_chat[sub_aug_idx + 1]\n",
    "\n",
    "    orig_chat, orig_aug_idx = row['chat'], row['aug_idx']\n",
    "    aug_chat = orig_chat[:orig_aug_idx + 1] + [sub_answer] + orig_chat[orig_aug_idx + 2:]\n",
    "\n",
    "    return aug_chat\n",
    "\n",
    "\n",
    "\n",
    "chats_df['aug_chat'] = chats_df.progress_apply(lambda x: substitue_sim_answer(x, chats_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats = np.array(chats_df[['chat', 'aug_chat']].values)\n",
    "\n",
    "to_shuffle = np.random.rand(len(chats)) > 0.5\n",
    "\n",
    "labels = np.where(to_shuffle, -1, 1)\n",
    "\n",
    "to_shuffle = np.column_stack((to_shuffle, to_shuffle))\n",
    "shuffled_chats = np.where(to_shuffle, chats[:, ::-1], chats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcing graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(shuffled_chats, f\"data/{filename}_nodes.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(labels, f\"data/{filename}_labels.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "def create_chat_graph(chat):\n",
    "    human_idxs = [i for i in range(0, len(chat), 2)]\n",
    "\n",
    "    chat_edges = []\n",
    "    chat_edges_idxs = []\n",
    "    for ui in range(len(chat)):\n",
    "        for uj in range(len(chat)):\n",
    "            if ui == uj:\n",
    "                edge_type = [True, False, False, False]\n",
    "            else:\n",
    "                edge_type = [\n",
    "                    False,\n",
    "                    ui > uj,\n",
    "                    ui in human_idxs,\n",
    "                    uj in human_idxs,\n",
    "                ]\n",
    "\n",
    "            edge_type = sum(2**i for i, v in enumerate(reversed(edge_type)) if v)\n",
    "\n",
    "            chat_edges_idxs.append((ui, uj))\n",
    "            chat_edges.append(edge_type)\n",
    "    \n",
    "    return chat_edges, chat_edges_idxs\n",
    "\n",
    "\n",
    "edges = []\n",
    "edge_idxs = []\n",
    "for c1, c2 in shuffled_chats:\n",
    "   c1_edges, c1_edge_idxs = create_chat_graph(c1)\n",
    "   c2_edges, c2_edge_idxs = create_chat_graph(c2)\n",
    "\n",
    "   edges.append(Tensor([c1_edges, c2_edges]).to(torch.int8))\n",
    "   edge_idxs.append(Tensor([c1_edge_idxs, c2_edge_idxs]).T.long())\n",
    "\n",
    "torch.save(edges, f\"data/{filename}_edges.pt\")\n",
    "torch.save(edge_idxs, f\"data/{filename}_edge_idxs.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
