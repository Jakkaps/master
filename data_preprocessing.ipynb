{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "filename = 'twitter_cs'\n",
    "\n",
    "twcs: pd.DataFrame = pd.read_csv(f'data/{filename}.csv', nrows=1_000)\n",
    "twcs = twcs.sort_values('created_at')\n",
    "twcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twcs[\"text\"] = (\n",
    "    twcs[\"text\"]\n",
    "    .str.replace(r\"^\\s*@[^ ]*\", \"\", regex=True)\n",
    "    .str.replace(r\"https?:\\/\\/[^\\s\\\\n]+\", \"\", regex=True)\n",
    "    .str.replace(r\"\\n+\", ' ', regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "twcs = twcs.rename(columns={'inbound': 'is_customer'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root(tweet_id, df):\n",
    "    parent_id = tweet_id\n",
    "\n",
    "    while True:\n",
    "        potential_parent = df[df['in_response_to_tweet_id'] == parent_id]['tweet_id']\n",
    "\n",
    "        if len(potential_parent) == 0:\n",
    "            return parent_id\n",
    "        \n",
    "        parent_id = potential_parent.values[0]\n",
    "\n",
    "tqdm.pandas(desc=\"Making threads...\")\n",
    "twcs['thread_id'] = twcs['tweet_id'].progress_apply(lambda x: find_root(x, twcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate to chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_conversations(df):\n",
    "    altnerating_messages = []\n",
    "    last_is_customer = None\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        is_customer = row['is_customer']\n",
    "        if last_is_customer == is_customer:\n",
    "            altnerating_messages[-1] += ' ' + row['text']\n",
    "        else:\n",
    "            altnerating_messages.append(row['text'])\n",
    "            last_is_customer = is_customer\n",
    "\n",
    "    return altnerating_messages\n",
    "\n",
    "chats = twcs.copy()\n",
    "tqdm.pandas(desc=\"Grouping conversations...\")\n",
    "chats['chat'] = chats.progress_apply(lambda x: group_conversations(twcs[twcs['thread_id'] == x['thread_id']]), axis=1)\n",
    "chats = chats.drop_duplicates('thread_id')[['chat']]\n",
    "chats['n_messages'] = chats['chat'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_length = chats['n_messages'] >= 4\n",
    "non_dm = chats['chat'].apply(lambda c: all([' dm' not in m.lower() for m in c]))\n",
    "\n",
    "chats = chats[proper_length & non_dm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "nodes = list(chats[\"chat\"])\n",
    "torch.save(nodes, f\"data/{filename}_nodes.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "edges = []\n",
    "edge_idxs = []\n",
    "for _, chat_row in chats.head(1).iterrows():\n",
    "    chat = chat_row[\"chat\"]\n",
    "\n",
    "    human_idxs = [i for i in range(0, len(chat), 2)]\n",
    "\n",
    "    chat_edges = []\n",
    "    chat_edges_idxs = []\n",
    "    for ui in range(len(chat)):\n",
    "        for uj in range(len(chat)):\n",
    "            if ui == uj:\n",
    "                edge_type = [True, False, False, False]\n",
    "            else:\n",
    "                edge_type = [\n",
    "                    False,\n",
    "                    ui > uj,\n",
    "                    ui in human_idxs,\n",
    "                    uj in human_idxs,\n",
    "                ]\n",
    "\n",
    "            edge_type = sum(2**i for i, v in enumerate(reversed(edge_type)) if v)\n",
    "\n",
    "            chat_edges_idxs.append((ui, uj))\n",
    "            chat_edges.append(edge_type)\n",
    "\n",
    "    edges.append(Tensor(chat_edges).to(torch.int8))\n",
    "    edge_idxs.append(Tensor(chat_edges_idxs).T.long())\n",
    "\n",
    "torch.save(edges, f\"data/{filename}_edges.pt\")\n",
    "torch.save(edge_idxs, f\"data/{filename}_edge_idxs.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
